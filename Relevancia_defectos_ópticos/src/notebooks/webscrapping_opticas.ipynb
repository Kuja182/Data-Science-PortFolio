{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En todos los webscrapping realizados hice una funcion que al meter directamente la url saliera la información.  \n",
    "  \n",
    "Aquí tuve varios problemas ya que en algunas páginas las direcciones concretas de cada dato estaban en diferentes posiciones, por ejemplo, optica universitaria y afflelou practicamente no tuve que hacer cambios en las funciones sin embargo en opticalia y greyhounders si.  \n",
    "  \n",
    "Aporto el codigo y las funciones, aún así, no ejecuteis el programa porque con bastante probabilidad no vais a obtener todos los datos de cada categoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('max_colwidth', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.afflelou.es/lentillas/\"\n",
    "response = requests.get(url)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = response.content\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_ou = {\"categoria\":[],\n",
    "         \"producto\":[],\n",
    "         \"remplazo\":[],\n",
    "         \"precio\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lc_gafas_ou(url): \n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "    \n",
    "    all_lentillas = soup.find_all(\"li\",class_=\"item last\")\n",
    "    \n",
    "    lc_ou = {\"nombre_completo\":[],\n",
    "        \"categoria\":[],\n",
    "         \"producto\":[],\n",
    "         \"reemplazo\":[],\n",
    "         \"precio\":[]}\n",
    "    for lentilla in all_lentillas:\n",
    "        lc_ou[\"nombre_completo\"].append((lentilla.find(\"a\").get(\"data-name\")))\n",
    "        lc_ou[\"categoria\"].append((lentilla.find(\"a\").get(\"data-category\")))\n",
    "        lc_ou[\"producto\"].append((lentilla.find(\"a\").get(\"data-brand\")))\n",
    "        lc_ou[\"reemplazo\"].append((lentilla.find(\"a\").get(\"data-list\")))\n",
    "        lc_ou[\"precio\"].append((lentilla.find(\"a\").get(\"data-price\")))\n",
    "    df_lc = pd.DataFrame(lc_ou)\n",
    "    df_lc.to_csv(\"Data/csv_scrap/\" + url.split(\"/\")[-1] + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gafas_greyhounders(url): \n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "    \n",
    "    gre_gf_lc = soup.find_all(\"li\",class_=\"grid__item col-6 col-sm-4\")\n",
    "    \n",
    "    lc__gf_grey = {\"nombre_completo\":[],\n",
    "                    \"precio\":[]}\n",
    "    for prod in gre_gf_lc:\n",
    "        lc__gf_grey[\"nombre_completo\"].append((prod.find(\"span\").get_text(\"visually-hidden\")))\n",
    "        lc__gf_grey[\"precio\"].append((prod.find(\"span\",class_=\"price-item price-item--regular\")).get_text())\n",
    "    df_gf_lc = pd.DataFrame(lc__gf_grey)\n",
    "    df_gf_lc.to_csv(\"Data/csv_scrap/\" + url.split(\"/\")[-1] + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lc_greyhounders(url): \n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "    \n",
    "    gre_gf_lc = soup.find_all(\"div\",class_=\"ProductCardstyled__ProductCardStyles-sc-1yccf7l-0 iFOwmD ProductCard\")\n",
    "    \n",
    "    lc__gf_grey = {\"nombre_completo\":[],\n",
    "                    \"precio\":[]}\n",
    "    for prod in gre_gf_lc:\n",
    "        lc__gf_grey[\"nombre_completo\"].append((prod.find(\"span\").get_text(\"visually-hidden\")))\n",
    "        lc__gf_grey[\"precio\"].append((prod.find(\"span\",class_=\"price-item price-item--sale\")).get_text())\n",
    "    df_gf_lc = pd.DataFrame(lc__gf_grey)\n",
    "    df_gf_lc.to_csv(\"Data/csv_scrap/\" + url.split(\"/\")[-1] + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lc_gafas_opticalia(url): \n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "    \n",
    "    all_lentillas = soup.find_all(\"div\",class_=\"ProductCardstyled__ProductCardStyles-sc-1yccf7l-0 kDLTuk ProductCard\")\n",
    "    \n",
    "    lc_ou = {\"nombre_completo\":[],\n",
    "            \"categoria\":[],\n",
    "            \"producto\":[],\n",
    "            \"reemplazo\":[],\n",
    "            \"precio\":[]}\n",
    "    for lentilla in all_lentillas:\n",
    "        lc_ou[\"nombre_completo\"].append((lentilla.find(\"p\",class_=\"ProductCard-Info-Line1-Title\")).get_text())\n",
    "        lc_ou[\"categoria\"].append(\"Lentillas\")\n",
    "        lc_ou[\"producto\"].append(\"Liquidos\")\n",
    "        lc_ou[\"reemplazo\"].append(\"Mensuales\")\n",
    "        lc_ou[\"precio\"].append((lentilla.find(\"div\",class_=\"ProductCard-Info-Line1-Price\")).get_text())\n",
    "    df_lc = pd.DataFrame(lc_ou)\n",
    "    df_lc.to_csv(\"Data/csv_scrap/grad.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lc_gafas_afflelou(url): \n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "    \n",
    "    all_lentillas = soup.find_all(\"div\",class_=\"col-lg-4 col-md-6 col-12 py-2 px-3\")\n",
    "    \n",
    "    lc_ou = {\"nombre_completo\":[],\n",
    "            \"categoria\":[],\n",
    "            \"precio\":[]}\n",
    "    for lentilla in all_lentillas:\n",
    "        lc_ou[\"nombre_completo\"].append((lentilla.find(\"span\",class_=\"c-product__description-item u-hyphen-initial d-block text-silver text-uppercase\")).get_text())\n",
    "        lc_ou[\"categoria\"].append(lentilla.find(\"span\",class_=\"c-product__description-item u-hyphen-initial d-block text-light\").get_text())\n",
    "        lc_ou[\"precio\"].append((lentilla.find(\"p\",class_=\"c-product__description-item c-product__description-item__price mt-2 mb-0 mx-0 text-silver is-active\")).get_text(\"span\"))\n",
    "    df_lc = pd.DataFrame(lc_ou)\n",
    "    df_lc.to_csv(\"Data/csv_scrap/grad.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
